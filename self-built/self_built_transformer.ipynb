{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization and Q&A Using Transformer\n",
    "\n",
    "- we need encoder-decoder architecture\n",
    "\n",
    "- Encoder is for\n",
    "\n",
    "- Decoder is for \n",
    "\n",
    "⸻\n",
    "\n",
    "1. Understand the Transformer Architecture\n",
    "\n",
    "At a high level, the original Transformer (from the 2017 paper) consists of:\n",
    "\t•\tInput Embedding (plus positional encoding)\n",
    "\t•\tEncoder Blocks (for encoding input)\n",
    "\t•\tDecoder Blocks (for generating output)\n",
    "\t•\tMulti-Head Self-Attention\n",
    "\t•\tFeedforward Neural Networks\n",
    "\t•\tLayer Normalization & Residual Connections\n",
    "\n",
    "⸻\n",
    "\n",
    "2. Minimal Setup (Start Here)\n",
    "\n",
    "Use PyTorch or TensorFlow — most people use PyTorch for scratch implementations.\n",
    "\n",
    "Basic Structure:\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(self, ...):\n",
    "        super().__init__()\n",
    "        self.encoder = Encoder(...)\n",
    "        self.decoder = Decoder(...)\n",
    "    \n",
    "    def forward(self, src, tgt):\n",
    "        encoder_output = self.encoder(src)\n",
    "        output = self.decoder(tgt, encoder_output)\n",
    "        return output\n",
    "\n",
    "⸻\n",
    "\n",
    "3. Core Components to Implement\n",
    "\t•\tPositional Encoding\n",
    "\t•\tMulti-Head Attention\n",
    "\t•\tLayer Normalization\n",
    "\t•\tFeed-Forward Layer\n",
    "\t•\tEncoder Layer (stacked N times)\n",
    "\t•\tDecoder Layer (stacked N times)\n",
    "\t•\tFinal Linear + Softmax for prediction\n",
    "\n",
    "⸻\n",
    "\n",
    "4. Helpful Resources\n",
    "\t•\tThe Annotated Transformer (PyTorch):\n",
    "\t•\thttps://nlp.seas.harvard.edu/2018/04/03/attention.html\n",
    "\t•\tTransformer from scratch in PyTorch (YouTube):\n",
    "The AI Epiphany channel is gold for this.\n",
    "\n",
    "⸻\n",
    "\n",
    "5. Training Your Transformer\n",
    "\n",
    "Once you build the model:\n",
    "\t•\tChoose your dataset (e.g., for Q&A, use SQuAD; for summarization, try CNN/DailyMail).\n",
    "\t•\tTokenize your data.\n",
    "\t•\tDefine your loss (usually CrossEntropyLoss for text).\n",
    "\t•\tUse teacher forcing during training.\n",
    "\n",
    "⸻\n",
    "\n",
    "Optional: Start Small\n",
    "\n",
    "You can start by building:\n",
    "\t•\tJust the Encoder (like BERT)\n",
    "\t•\tOr a small Encoder-Decoder (like TinyT5)\n",
    "\n",
    "⸻"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TODO:\n",
    "\n",
    "- collect data:\n",
    "    - collect all the ids\n",
    "    - use https://arxiv.org/pdf/{id} to download pdf\n",
    "    - read and save all the pdf articles in to csv file.\n",
    "\n",
    "- which part of the transformer is controlling summarization vs translation?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import kagglehub\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/Cornell-University/arxiv?dataset_version_number=225...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1.42G/1.42G [00:39<00:00, 38.1MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/seaqueue/.cache/kagglehub/datasets/Cornell-University/arxiv/versions/225\n"
     ]
    }
   ],
   "source": [
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"Cornell-University/arxiv\")\n",
    "\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['arxiv-metadata-oai-snapshot.json']\n"
     ]
    }
   ],
   "source": [
    "files = os.listdir(path)\n",
    "print(files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 5 records:           id           submitter  \\\n",
      "0  0704.0001      Pavel Nadolsky   \n",
      "1  0704.0002        Louis Theran   \n",
      "2  0704.0003         Hongjun Pan   \n",
      "3  0704.0004        David Callan   \n",
      "4  0704.0005  Alberto Torchinsky   \n",
      "\n",
      "                                             authors  \\\n",
      "0  C. Bal\\'azs, E. L. Berger, P. M. Nadolsky, C.-...   \n",
      "1                    Ileana Streinu and Louis Theran   \n",
      "2                                        Hongjun Pan   \n",
      "3                                       David Callan   \n",
      "4           Wael Abu-Shammala and Alberto Torchinsky   \n",
      "\n",
      "                                               title  \\\n",
      "0  Calculation of prompt diphoton production cros...   \n",
      "1           Sparsity-certifying Graph Decompositions   \n",
      "2  The evolution of the Earth-Moon system based o...   \n",
      "3  A determinant of Stirling cycle numbers counts...   \n",
      "4  From dyadic $\\Lambda_{\\alpha}$ to $\\Lambda_{\\a...   \n",
      "\n",
      "                                  comments  \\\n",
      "0  37 pages, 15 figures; published version   \n",
      "1    To appear in Graphs and Combinatorics   \n",
      "2                      23 pages, 3 figures   \n",
      "3                                 11 pages   \n",
      "4                                     None   \n",
      "\n",
      "                                 journal-ref                         doi  \\\n",
      "0                   Phys.Rev.D76:013009,2007  10.1103/PhysRevD.76.013009   \n",
      "1                                       None                        None   \n",
      "2                                       None                        None   \n",
      "3                                       None                        None   \n",
      "4  Illinois J. Math. 52 (2008) no.2, 681-689                        None   \n",
      "\n",
      "          report-no       categories  \\\n",
      "0  ANL-HEP-PR-07-12           hep-ph   \n",
      "1              None    math.CO cs.CG   \n",
      "2              None   physics.gen-ph   \n",
      "3              None          math.CO   \n",
      "4              None  math.CA math.FA   \n",
      "\n",
      "                                             license  \\\n",
      "0                                               None   \n",
      "1  http://arxiv.org/licenses/nonexclusive-distrib...   \n",
      "2                                               None   \n",
      "3                                               None   \n",
      "4                                               None   \n",
      "\n",
      "                                            abstract  \\\n",
      "0    A fully differential calculation in perturba...   \n",
      "1    We describe a new algorithm, the $(k,\\ell)$-...   \n",
      "2    The evolution of Earth-Moon system is descri...   \n",
      "3    We show that a determinant of Stirling cycle...   \n",
      "4    In this paper we show how to compute the $\\L...   \n",
      "\n",
      "                                            versions update_date  \\\n",
      "0  [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...  2008-11-26   \n",
      "1  [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  2008-12-13   \n",
      "2  [{'version': 'v1', 'created': 'Sun, 1 Apr 2007...  2008-01-13   \n",
      "3  [{'version': 'v1', 'created': 'Sat, 31 Mar 200...  2007-05-23   \n",
      "4  [{'version': 'v1', 'created': 'Mon, 2 Apr 2007...  2013-10-15   \n",
      "\n",
      "                                      authors_parsed  \n",
      "0  [[Balázs, C., ], [Berger, E. L., ], [Nadolsky,...  \n",
      "1           [[Streinu, Ileana, ], [Theran, Louis, ]]  \n",
      "2                                 [[Pan, Hongjun, ]]  \n",
      "3                                [[Callan, David, ]]  \n",
      "4  [[Abu-Shammala, Wael, ], [Torchinsky, Alberto, ]]  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(path+'/arxiv-metadata-oai-snapshot.json', lines=True)\n",
    "print(\"First 5 records:\", df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax\n",
    "import copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    # TODO:\n",
    "    # what is generator?\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "    \n",
    "    # what are these masks for why do we need two masks?\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        # take in and process maksed src and tgt sequences\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Standard linear + softmax generation step\n",
    "    '''\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: which activation function is the best for summarization?\n",
    "        return log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    # Produce N identical layers\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        'pass the input (and mask) through each layer in turn'\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    '''Construct a layernorm module (See citation for details).'''\n",
    "    # TODO: what is the purpose of eps?\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
