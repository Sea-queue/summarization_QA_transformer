{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarization and Q&A Using Transformer\n",
    "\n",
    "- we need encoder-decoder architecture\n",
    "- Helpful Resources\n",
    "\t•\tThe Annotated Transformer (PyTorch):\n",
    "\t•\thttps://nlp.seas.harvard.edu/2018/04/03/attention.html\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO:\n",
    "- collect data:\n",
    "    - collect all the ids\n",
    "    - use https://arxiv.org/pdf/{id} to download pdf\n",
    "    - read and save all the pdf articles in to csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn.functional import log_softmax\n",
    "import copy\n",
    "import math\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download latest version\n",
    "# path = kagglehub.dataset_download(\"Cornell-University/arxiv\")\n",
    "# files = os.listdir(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df = pd.read_json(path+'/arxiv-metadata-oai-snapshot.json', lines=True)\n",
    "# print(\"First 5 records:\", df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Architecture\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderDecoder(nn.Module):\n",
    "    def __init__(self, encoder, decoder, src_embed, tgt_embed, generator):\n",
    "        super(EncoderDecoder, self).__init__()\n",
    "        self.encoder = encoder\n",
    "        self.decoder = decoder\n",
    "        self.src_embed = src_embed\n",
    "        self.tgt_embed = tgt_embed\n",
    "        self.generator = generator\n",
    "    \n",
    "    # what are these masks for why do we need two masks?\n",
    "    def forward(self, src, tgt, src_mask, tgt_mask):\n",
    "        # take in and process maksed src and tgt sequences\n",
    "        return self.decode(self.encode(src, src_mask), src_mask, tgt, tgt_mask)\n",
    "    \n",
    "    def encode(self, src, src_mask):\n",
    "        return self.encoder(self.src_embed(src), src_mask)\n",
    "    \n",
    "    def decode(self, memory, src_mask, tgt, tgt_mask):\n",
    "        return self.decoder(self.tgt_embed(tgt), memory, src_mask, tgt_mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Generator(nn.Module):\n",
    "    '''\n",
    "    Standard linear + softmax generation step\n",
    "    '''\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Generator, self).__init__()\n",
    "        self.proj = nn.Linear(d_model, vocab)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # TODO: which activation function is the best for summarization?\n",
    "        return log_softmax(self.proj(x), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clones(module, N):\n",
    "    # Produce N identical layers\n",
    "    return nn.ModuleList([copy.deepcopy(module) for _ in range(N)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNorm(nn.Module):\n",
    "    '''Construct a layernorm module (See citation for details).'''\n",
    "    # TODO: what is the purpose of eps?\n",
    "    def __init__(self, features, eps=1e-6):\n",
    "        super(LayerNorm, self).__init__()\n",
    "        self.a_2 = nn.Parameter(torch.ones(features))\n",
    "        self.b_2 = nn.Parameter(torch.zeros(features))\n",
    "        self.eps = eps\n",
    "    \n",
    "    def forward(self, x):\n",
    "        mean = x.mean(-1, keepdim=True)\n",
    "        std = x.std(-1, keepdim=True)\n",
    "        return self.a_2 * (x - mean) / (std + self.eps) + self.b_2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "        \n",
    "    def forward(self, x, mask):\n",
    "        'pass the input (and mask) through each layer in turn'\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, mask)\n",
    "        return self.norm(x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TODO: what is this function trying to do? what is the input and output?\n",
    "class SublayerConnection(nn.Module):\n",
    "    def __init__(self, size, dropout):\n",
    "        super(SublayerConnection, self).__init__()\n",
    "        self.norm = LayerNorm(size)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x, sublayer):\n",
    "        # why add X to the tensor?\n",
    "        return x + self.dropout(sublayer(self.norm(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.self_attn = self_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "        self.size = size\n",
    "\n",
    "    def forward(self, x, mask):\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "        return self.sublayer[1](x, self.feed_forward)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    def __init__(self, layer, N):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.layers = clones(layer, N)\n",
    "        self.norm = LayerNorm(layer.size)\n",
    "\n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        for layer in self.layers:\n",
    "            x = layer(x, memory, src_mask, tgt_mask)\n",
    "        return self.norm(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(nn.Module):\n",
    "    def __init__(self, size, self_attn, src_attn, feed_forward, dropout):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.size = size\n",
    "        self.self_attn = self_attn\n",
    "        self.src_attn = src_attn\n",
    "        self.feed_forward = feed_forward\n",
    "        self.sublayer = clones(SublayerConnection(size, dropout), 3)\n",
    "    \n",
    "    def forward(self, x, memory, src_mask, tgt_mask):\n",
    "        m = memory\n",
    "        x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, tgt_mask))\n",
    "        x = self.sublayer[1](x, lambda x: self.src_attn(x, m, m, src_mask))\n",
    "        return self.sublayer[2](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def subsequent_mask(size):\n",
    "    attn_shape = (1, size, size)\n",
    "    subsequent_mask = torch.triu(torch.ones(attn_shape), diagonal=1).type(torch.uint8)\n",
    "\n",
    "    return subsequent_mask == 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def attention(query, key, value, mask=None, dropout=None):\n",
    "    d_k = query.size(-1)\n",
    "    scores = torch.matmul(query, key.transpose(-2, -1)) / math.sqrt(d_k)\n",
    "    if mask is not None:\n",
    "        scores = scores.masked_fill(mask == 0, -1e9)\n",
    "    p_attn = torch.softmax(scores, dim=-1)\n",
    "\n",
    "    if dropout is not None:\n",
    "        p_attn = dropout(p_attn)\n",
    "    return torch.matmul(p_attn, value), p_attn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiHeadedAttention(nn.Module):\n",
    "    def __init__(self, h, d_model, dropout = 0.1):\n",
    "        super(MultiHeadedAttention, self).__init__()\n",
    "        assert d_model % h == 0\n",
    "        self.d_k = d_model // h\n",
    "        self.h = h\n",
    "        self.linears = clones(nn.Linear(d_model, d_model), 4)\n",
    "        self.attn = None\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, query, key, value, mask=None):\n",
    "        if mask is not None:\n",
    "            mask = mask.unsqueeze(1)\n",
    "        nbatches = query.size(0)\n",
    "\n",
    "        query, key, value = [\n",
    "            l(x).view(nbatches, -1, self.h, self.d_k).transpose(1, 2)\n",
    "            for l, x in zip(self.linears, (query, key, value))\n",
    "        ]\n",
    "\n",
    "        x, self.attn = attention(query, key, value, mask=mask, dropout=self.dropout)\n",
    "\n",
    "        x = (x.transpose(1, 2).contiguous().view(nbatches, -1, self.h * self.d_k))\n",
    "\n",
    "        del query\n",
    "        del key\n",
    "        del value\n",
    "\n",
    "        return self.linears[-1](x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionwiseFeedForward(nn.Module):\n",
    "    def __init__(self, d_model, d_ff, dropout=0.1):\n",
    "        super(PositionwiseFeedForward, self).__init__()\n",
    "        self.w_1 = nn.Linear(d_model, d_ff)\n",
    "        self.w_2 = nn.Linear(d_ff, d_model)\n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.w_2(self.dropout(self.w_1(x).relu()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(nn.Module):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        self.lut = nn.Embedding(vocab, d_model)\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.lut(x) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PositionalEncoding(nn.Module):\n",
    "    def __init__(self, d_model, dropout=0.1, max_len=5000):\n",
    "        super(PositionalEncoding, self).__init__()\n",
    "        self.dropout = nn.Dropout(p=dropout)\n",
    "\n",
    "        pe = torch.zeros(max_len, d_model)\n",
    "        position = torch.arange(0, max_len, dtype=torch.float).unsqueeze(1)\n",
    "        div_term = torch.exp(torch.arange(0, d_model, 2) * -(math.log(10000.0) / d_model))\n",
    "        pe[:, 0::2] = torch.sin(position * div_term)\n",
    "        pe[:, 1::2] = torch.cos(position * div_term)\n",
    "        pe = pe.unsqueeze(0)\n",
    "        self.register_buffer('pe', pe)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = x + self.pe[:, : x.size(1)].requires_grad_(False)\n",
    "        return self.dropout(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_model(src_vocab, tgt_vocab, N=6, d_model= 512, d_ff=2048, h=8, dropout=0.1):\n",
    "    c = copy.deepcopy\n",
    "    attn = MultiHeadedAttention(h, d_model)\n",
    "    ff = PositionwiseFeedForward(d_model, d_ff, dropout)\n",
    "    position = PositionalEncoding(d_model, dropout)\n",
    "    model = EncoderDecoder(\n",
    "        Encoder(EncoderLayer(d_model, c(attn), c(ff), dropout), N),\n",
    "        Decoder(DecoderLayer(d_model, c(attn), c(attn), c(ff), dropout), N),\n",
    "        nn.Sequential(Embeddings(d_model, src_vocab), c(position)),\n",
    "        nn.Sequential(Embeddings(d_model, tgt_vocab), c(position)),\n",
    "        Generator(d_model, tgt_vocab)\n",
    "    )\n",
    "    for p in model.parameters():\n",
    "        if p.dim() > 1:\n",
    "            nn.init.xavier_uniform_(p)\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def inference_test():\n",
    "    test_model = make_model(11, 11, 2)\n",
    "    test_model.eval()\n",
    "    src = torch.LongTensor([[1, 2, 3, 4, 5, 6, 7, 8, 9, 10]])\n",
    "    src_mask = torch.ones(1, 1, 10)\n",
    "\n",
    "    memory = test_model.encode(src, src_mask)\n",
    "    ys = torch.zeros(1, 1).type_as(src.data)\n",
    "\n",
    "    for i in range(9):\n",
    "        out = test_model.decode(memory, src_mask, ys, subsequent_mask(ys.size(1)).type_as(src.data))\n",
    "        prob = test_model.generator(out[:, -1])\n",
    "        _, next_word = torch.max(prob, dim=1)\n",
    "        next_word = next_word.data[0]\n",
    "        ys = torch.cat([ys, torch.empty(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    \n",
    "    print(\"Example Untrained Model Prediction:\", ys)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Example Untrained Model Prediction: tensor([[0, 5, 3, 3, 3, 3, 3, 3, 3, 3]])\n",
      "Example Untrained Model Prediction: tensor([[0, 5, 5, 5, 5, 5, 5, 5, 5, 5]])\n",
      "Example Untrained Model Prediction: tensor([[0, 1, 5, 7, 5, 7, 5, 7, 5, 7]])\n",
      "Example Untrained Model Prediction: tensor([[0, 1, 7, 4, 3, 0, 1, 7, 2, 5]])\n",
      "Example Untrained Model Prediction: tensor([[0, 8, 6, 8, 6, 8, 6, 8, 6, 8]])\n",
      "Example Untrained Model Prediction: tensor([[0, 3, 8, 2, 9, 2, 9, 2, 9, 2]])\n",
      "Example Untrained Model Prediction: tensor([[0, 2, 6, 2, 6, 2, 6, 2, 6, 2]])\n",
      "Example Untrained Model Prediction: tensor([[ 0,  6,  6,  6,  6,  6, 10,  6,  6,  6]])\n",
      "Example Untrained Model Prediction: tensor([[0, 8, 4, 5, 5, 5, 1, 1, 1, 1]])\n",
      "Example Untrained Model Prediction: tensor([[0, 6, 8, 4, 4, 3, 1, 7, 1, 6]])\n"
     ]
    }
   ],
   "source": [
    "for _ in range (10):\n",
    "    inference_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data preparation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "billsum = load_dataset(\"billsum\", split=\"ca_test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sample {'text': 'The people of the State of California do enact as follows:\\n\\n\\nSECTION 1.\\nSection 35554 of the Vehicle Code, as amended by Section 2 of Chapter 263 of the Statutes of 2014, is amended to read:\\n35554.\\n(a) (1) Notwithstanding Section 35550, the maximum gross weight on any one axle of a bus shall not exceed 20,500 pounds.\\n(2) This subdivision does not apply to a transit bus procured through a solicitation process pursuant to which a solicitation was issued before January 1, 2016. This subdivision does not apply to a bus purchased during an option period in a multiyear contract to purchase transit buses that is entered into before January 1, 2016, by a publicly owned or operated transit system, or an operator of a transit system under contract with a publicly owned or operated transit system, provided, however, that the option period does not exceed five years from the date of the original contract, or extend beyond January 1, 2021, whichever is earlier.\\n(b) A transit bus is not subject to Section 35550.\\n(c) Notwithstanding subdivision (a), the following provisions shall apply to a transit bus:\\n(1) The curb weight on any one axle of a transit bus procured through a solicitation process pursuant to which a solicitation was issued between January 1, 2016, and December 31, 2018, inclusive, shall not exceed 23,000 pounds.\\n(2) The curb weight on any one axle of a transit bus procured through a solicitation process pursuant to which a solicitation was issued on or after January 1, 2019, shall not exceed 22,000 pounds.\\n(d) Notwithstanding subdivisions (a) and (c), the following provisions shall apply to an articulated transit bus or zero-emission transit bus:\\n(1) The curb weight on any one axle of an articulated transit bus or zero-emission transit bus procured through a solicitation process pursuant to which a solicitation was issued between January 1, 2016, and December 31, 2017, inclusive, shall not exceed 25,000 pounds.\\n(2) The curb weight on any one axle of an articulated transit bus or zero-emission transit bus procured through a solicitation process pursuant to which a solicitation was issued between January 1, 2018, and December 31, 2019, inclusive, shall not exceed 24,000 pounds.\\n(3) The curb weight on any one axle of an articulated transit bus or zero-emission transit bus procured through a solicitation process pursuant to which a solicitation was issued between January 1, 2020, and December 31, 2021, inclusive, shall not exceed 23,000 pounds.\\n(4) The curb weight on any one axle of an articulated transit bus or zero-emission transit bus procured through a solicitation process pursuant to which a solicitation was issued on or after January 1, 2022, shall not exceed 22,000 pounds.\\n(e) Nothing in this article shall be construed to authorize a vehicle described in paragraph (2) of subdivision (a) or described in subdivision (c) or (d) to be operated in violation of Section 35753.\\n(f) A transit operator operating an articulated transit bus shall, by July 1, 2016, provide notice to all cities and counties in whose jurisdiction the bus will operate in the upcoming calendar year, identifying the approximate routes upon which the bus is expected to be scheduled for service, including the names of streets and roads upon which that service is likely to take place. Thereafter, a transit operator operating an articulated transit bus shall annually provide notice by July 1, to all cities and counties in whose jurisdiction the bus will operate in the upcoming calendar year, identifying any changes to the service on those routes and any new routes upon which the bus is expected to be scheduled for the upcoming year. The notice shall include data from information provided by the bus manufacturer to the transit operator, identifying the weight of the articulated bus.\\n(g) For purposes of this section, the term “curb weight” means the total weight of a fully loaded transit bus, including maximum fuel, oil, and coolant, and all equipment used in the normal operation of the bus, except without passengers or a driver.\\n(h) Notwithstanding subdivisions (a) to (g), inclusive, a transit bus shall not operate on the Dwight D. Eisenhower System of Interstate and Defense Highways in excess of the weight limitation for transit buses specified in federal law.\\n(i) If the gross weight imposed upon the highway by the wheels on any one axle of a transit bus exceeds 20,000 pounds, the axle shall be supported by four wheels bearing load upon the highway.\\nSEC. 2.\\nNo reimbursement is required by this act pursuant to Section 6 of Article XIII\\u2009B of the California Constitution because the only costs that may be incurred by a local agency or school district will be incurred because this act creates a new crime or infraction, eliminates a crime or infraction, or changes the penalty for a crime or infraction, within the meaning of Section 17556 of the Government Code, or changes the definition of a crime within the meaning of Section 6 of Article XIII\\u2009B of the California Constitution.', 'summary': 'Existing law, operative January 1, 2016, provides that the gross weight on any one axle of a bus shall not exceed 20,500 pounds. Existing law exempts from this limitation a transit bus procured through a solicitation process pursuant to which a solicitation was issued before January 1, 2013. A violation of this provision is a crime.\\nThis bill would exempt from the weight limitation transit buses procured through a solicitation process pursuant to which a solicitation was issued before January 1, 2016. The bill would provide that the weight limitation would not apply to a bus purchased during an option period in a multiyear contract to purchase transit buses that is entered into before January 1, 2016, by a publicly owned or operated transit system, or an operator of a transit system under contract with a publicly owned or operated transit system, provided that the option period does not exceed 5 years from the date of the original contract, or extend beyond January 1, 2021, whichever is earlier. This bill would also establish certain weight limitations for transit buses procured through a solicitation process pursuant to which a solicitation was issued at a specified time. The bill would provide that these provisions do not authorize the operation of a transit bus on a bridge or certain other structures if the gross weight of the transit bus is greater than the maximum weight which the bridge or other structure can safely sustain. The bill would require, if the gross weight imposed upon the highway by the wheels on any one axle of a transit bus exceeds 20,000 pounds, the axle to be supported by 4 wheels bearing load upon the highway. Because a violation of these provisions would be a crime, this bill would impose a state-mandated local program.\\nThe bill would require a transit operator operating an articulated bus to provide notice, by July 1, 2016, to all cities and counties in whose jurisdiction the bus will operate in the upcoming calendar year, of the approximate routes upon which the bus will operate. The bill would also require an annual notice by July 1 thereafter to all cities and counties under whose jurisdiction the bus will operate, identifying any changes to the service on those routes or any new routes upon which the bus is expected to be scheduled for the upcoming calendar year.\\nThe California Constitution requires the state to reimburse local agencies and school districts for certain costs mandated by the state. Statutory provisions establish procedures for making that reimbursement.\\nThis bill would provide that no reimbursement is required by this act for a specified reason.', 'title': 'An act to amend Section 35554 of the Vehicle Code, relating to vehicles.'}\n",
      "training dataset size: 989\n",
      "testing dataset size: 248\n"
     ]
    }
   ],
   "source": [
    "# Split data to train(0.8) & test(0.2)\n",
    "billsum_split = billsum.train_test_split(test_size=0.2)\n",
    "print('sample', billsum_split[\"train\"][0])\n",
    "print('training dataset size:', len(billsum_split['train']))\n",
    "print('testing dataset size:', len(billsum_split['test']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### data tokenization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "checkpoint = \"google-t5/t5-small\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
    "\n",
    "prefix = \"summarize: \"\n",
    "\n",
    "def preprocess_function(examples):\n",
    "    inputs = [prefix + doc for doc in examples[\"text\"]]\n",
    "    model_inputs = tokenizer(inputs, max_length=1024, truncation=True)\n",
    "    labels = tokenizer(text_target=examples[\"summary\"], max_length=128, truncation=True)\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 989/989 [00:02<00:00, 415.65 examples/s]\n",
      "Map: 100%|██████████| 248/248 [00:00<00:00, 439.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "tokenized_billsum = billsum_split.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " {'train': (989, 6), 'test': (248, 6)}\n",
      " dict_keys(['text', 'summary', 'title', 'input_ids', 'attention_mask', 'labels'])\n"
     ]
    }
   ],
   "source": [
    "print('', tokenized_billsum.shape)\n",
    "print('', tokenized_billsum['test'][0].keys())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " [21603, 10, 37, 151, 13, 8, 1015, 13, 1826, 103, 3, 35, 2708, 38, 6963, 10, 180, 3073, 9562, 1300, 7491, 209, 41, 287, 526, 4733, 28, 5568, 3, 15442, 11434, 61, 19, 974, 12, 8647, 314, 13, 2733, 209, 13, 6022, 3, 17864, 13, 8, 1685, 11, 6859, 3636, 6, 12, 608, 10, 7491, 1300, 23549, 5, 3, 15442, 11434, 5, 37, 28204, 12902, 11, 15884, 7, 66, 13, 8, 826, 10, 41, 9, 61, 94, 19, 2196, 24, 80, 16, 2391, 3165, 1826, 29, 7, 65, 8363, 6, 11, 8, 2302, 33, 6937, 7313, 5, 37, 1805, 381, 13, 273, 3, 2544, 1342, 33, 4161, 57, 8363, 19, 7752, 11, 5024, 12, 36, 231, 1146, 116, 2945, 53, 16, 8, 20588, 13, 686, 209, 8363, 11, 73, 25930, 18716, 26, 7671, 257, 138, 8363, 5, 41, 115, 61, 1826, 65, 8, 4016, 381, 13, 2041, 126, 1488, 13, 8363, 16, 8, 907, 1323, 5, 41, 75, 61, 37, 20588, 13, 8363, 859, 7, 17, 66, 1826, 29, 7, 65, 1936, 3538, 1093, 147, 8, 657, 5112, 5, 41, 26, 61, 2035, 209, 14912, 770, 151, 16, 1826, 43, 4880, 18999, 346, 1422, 6, 3, 9, 1706, 24, 19, 3, 9, 30073, 12, 423, 3, 26558, 686, 204, 8363, 5, 100, 6490, 24, 8, 792, 2074, 13, 273, 12223, 56, 916, 12, 3098, 16, 8, 8605, 13, 14418, 5, 41, 15, 61, 37, 24753, 13, 12223, 7671, 257, 138, 8363, 16, 1826, 65, 1936, 1640, 1093, 16, 131, 2391, 203, 6, 45, 3, 19660, 1093, 13, 2833, 23113, 16, 6260, 12, 3, 26627, 1093, 13, 2833, 23113, 16, 3105, 6, 28, 8, 2822, 1166, 7, 21, 14326, 4330, 11, 19715, 3, 17211, 24, 8, 8209, 1080, 228, 661, 38, 306, 38, 12265, 519, 1093, 5, 41, 89, 61, 37, 5043, 1113, 12, 8, 1015, 13, 1826, 6, 379, 792, 533, 124, 11, 1341, 1358, 21, 8, 1058, 13, 8363, 6, 47, 147, 25264, 5, 1298, 2108, 16, 8574, 41, 122, 61, 290, 19, 3, 9, 3, 31350, 24753, 13, 686, 204, 8363, 859, 1826, 29, 7, 113, 33, 1589, 6, 978, 2837, 447, 6, 42, 13, 6578, 5233, 3, 2172, 12, 8, 879, 2074, 5, 282, 13, 8693, 8, 20588, 13, 8363, 859, 1589, 11, 978, 2837, 447, 151, 47, 2111, 1486, 24, 859, 529, 18, 12146, 14147, 2532, 1945, 7, 44, 3241, 968, 1093, 5, 6578, 7, 11, 5824, 2834, 277, 6, 16, 8, 12955, 6, 351, 1146, 1917, 13, 8363, 145, 119, 11683, 5, 18264, 1637, 441, 8, 6578, 11, 5824, 2834, 49, 2074, 351, 8, 2030, 24753, 11, 1020, 1879, 6, 379, 27422, 6, 1013, 6578, 7, 6, 11, 5824, 2834, 277, 6, 113, 5696, 45, 8363, 44, 1917, 13, 627, 1093, 6, 898, 1093, 6, 11, 72, 145, 507, 1093, 6, 6898, 5, 41, 107, 61, 71, 1100, 810, 13, 3, 9, 508, 538, 28, 3, 9, 108, 172, 179, 8363, 2074, 435, 24, 8, 1080, 13, 12223, 8363, 16, 24, 538, 22, 7, 19663, 2074, 19, 2111, 1486, 24, 13, 165, 879, 2074, 5, 41, 23, 61, 290, 19, 150, 10620, 21, 136, 686, 13, 8363, 117, 983, 6, 132, 19, 2084, 24, 8363, 54, 36, 19653, 42, 16124, 16, 3, 26558, 190, 4026, 1112, 11, 1035, 7897, 5, 41, 354, 61, 23549, 6, 116, 646, 73, 19234, 6, 54, 991, 12, 2261, 11, 11855, 14497, 11, 3, 9, 3915, 27617, 5, 41, 157, 61, 1404, 13, 175, 2261, 14497, 54, 36, 16124, 42, 16652, 28, 10063, 8209, 6, 1231, 1868, 1044, 18, 2864, 6, 11, 3798, 569, 4349, 5, 41, 40, 61, 94, 19, 8, 9508, 13, 8, 28204, 12, 1457, 8, 1015, 1775, 13, 2575, 1685, 12, 370, 12, 8, 28204, 251, 6, 379, 8, 2041, 2822, 1166, 7, 21, 14326, 4330, 11, 19715, 2188, 934, 6, 30, 8363, 9793, 11, 758, 1087, 4468, 57, 8, 1015, 1775, 13, 2575, 1685, 11, 15700, 7, 1968, 28, 8363, 9793, 11, 758, 1087, 5, 506, 1087, 33, 356, 7444, 57, 8, 1015, 1775, 13, 2575, 1685, 16, 8, 1826, 17092, 2926, 1412, 11, 8, 934, 3, 14134, 1600, 1412, 7201, 105, 279, 450, 537, 13, 23549, 16, 1826, 1239, 335, 4165, 5553, 5, 41, 9, 61, 37, 1015, 1775, 13, 2575, 1685, 1522, 4237, 3, 9, 934, 12, 8, 28204, 30, 42, 274, 1762, 1914, 7887, 24, 963, 3, 9, 9251, 11, 23436, 13, 5719, 30, 8363, 9793, 11, 758, 6, 3, 99, 136, 6, 45, 66, 13, 8, 826, 2836, 10, 5637, 37, 636, 13, 1826, 5, 6499, 37, 2822, 1166, 7, 21, 14326, 4330, 11, 19715, 5, 10153, 37, 1826, 17092, 2926, 5, 3, 10820, 2502, 3, 7, 28631, 8363, 8474, 5235, 1637, 5, 3, 15757, 2502, 12311, 4313, 57, 8, 3066, 38, 578, 2193, 7469, 11, 5719, 5, 41, 115, 61, 37, 3066, 1522, 560, 16, 8, 934, 136, 5719, 45, 273, 4222, 30, 66, 13, 8, 826, 1173, 10, 5637, 24122, 18, 390, 3266, 12, 1709, 42, 1865, 8363, 5, 6499, 389, 1693, 13, 8, 981, 1113, 8363, 11, 165, 14497, 43, 30, 8, 538, 5, 10153, 7587, 5719, 21, 8, 9793, 11, 758, 13, 8363, 5, 41, 75, 61, 37, 3066, 1522, 92, 560, 16, 8, 934, 3, 9, 4210, 13, 8, 1895, 593, 13, 13954, 344, 538, 10521, 28, 3553, 12, 478, 4992, 1087, 11, 8, 6537, 13, 251, 12, 8, 452, 1918, 5037, 11, 3, 13494, 8363, 11, 165, 14497, 5, 41, 26, 61, 8192, 4733, 1718, 1914, 4791, 8, 3066, 1522, 10943, 442, 66, 13, 8, 826, 251, 30, 165, 1284, 1620, 353, 10, 5637, 71, 9251, 13, 8, 866, 11, 1391, 13, 136, 3135, 6640, 12, 8, 3066, 21, 1356, 11, 1087, 3, 8287, 44, 3, 13494, 42, 5037, 8363, 5, 6499, 71, 9251, 13, 8, 15700, 7, 57, 8, 3066, 30, 1356, 11, 1087, 3, 8287, 44, 3, 13494, 42, 5037, 8363, 5, 41, 15, 61, 5637, 37, 5971, 21, 3, 14975, 3, 9, 934, 3, 16068, 365, 27444, 41, 9, 61, 19, 16, 11480, 30, 1762, 1914, 460, 2266, 5, 6499, 37, 934, 5776, 12, 8, 28204, 19890, 288, 12, 48, 1375, 1522, 36, 5776, 16, 1]\n",
      " [17061, 53, 973, 4797, 15, 7, 8, 1015, 1775, 13, 2575, 1685, 11, 3369, 7444, 165, 11552, 11, 9353, 3, 20651, 12, 6, 859, 119, 378, 6, 9932, 6, 3, 22140, 6, 11, 3, 22235, 452, 533, 6, 379, 1028, 7, 15, 1109, 1014, 251, 1918, 6716, 5, 100, 2876, 133, 1457, 8, 1015, 1775, 13, 2575, 1685, 12, 4237, 3, 9, 934, 12, 8, 28204, 30, 42, 274, 1762, 1914, 7887, 24, 963, 3, 9, 9251, 11, 23436, 13, 5719, 6, 38, 7173, 6, 30, 8363, 9793, 11, 758, 45, 824, 2836, 6, 379, 8, 636, 13, 1826, 11, 8, 2822, 1166, 7, 21, 14326, 4330, 11, 19715, 5, 37, 2876, 133, 1457, 8, 3066, 12, 6, 30866, 53, 1718, 1914, 4791, 10943, 442, 30, 165, 1]\n"
     ]
    }
   ],
   "source": [
    "print('', tokenized_billsum['test'][0]['input_ids'])\n",
    "print('', tokenized_billsum['test'][0]['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "48464\n",
      "23619\n"
     ]
    }
   ],
   "source": [
    "src_vocab = [item['text'].split() for item in tokenized_billsum['train']]\n",
    "flattened_src = set([item for sublist in src_vocab for item in sublist])\n",
    "print(len(flattened_src))\n",
    "\n",
    "target_vocab = [item['text'].split() for item in tokenized_billsum['test']]\n",
    "flattened_target = set([item for sublist in target_vocab for item in sublist])\n",
    "print(len(flattened_target))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<pad>\n"
     ]
    }
   ],
   "source": [
    "tokens = tokenizer.convert_ids_to_tokens(tokenized_billsum['train'][0]['input_ids'])\n",
    "# tokens = tokenizer.convert_ids_to_tokens([0])\n",
    "# Join tokens without any separator\n",
    "joined_text = ''.join(tokens)\n",
    "# Replace the marker with a space and strip any leading/trailing spaces\n",
    "final_text = joined_text.replace('▁', ' ').strip()\n",
    "print(final_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "989\n",
      "989\n"
     ]
    }
   ],
   "source": [
    "# Batching the data\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.nn.utils.rnn import pad_sequence\n",
    "\n",
    "input_tokens_src = [input_ids['input_ids'] for input_ids in tokenized_billsum['train']]\n",
    "input_tokens_target = [label['labels'] for label in tokenized_billsum['train']]\n",
    "print(len(input_tokens_src))\n",
    "print(len(input_tokens_target))\n",
    "\n",
    "# Suppose input_tokens_src and input_tokens_target are lists of lists of token IDs.\n",
    "# For example:\n",
    "# input_tokens_src = [[1, 2, 3], [4, 5]]\n",
    "# input_tokens_target = [[6, 7, 8], [9, 10]]\n",
    "\n",
    "# Convert each list into a tensor\n",
    "src_tensors = [torch.tensor(seq, dtype=torch.long) for seq in input_tokens_src]\n",
    "target_tensors = [torch.tensor(seq, dtype=torch.long) for seq in input_tokens_target]\n",
    "\n",
    "# Pad the sequences (e.g., using padding_value=0, and setting batch_first=True)\n",
    "padded_src = pad_sequence(src_tensors, batch_first=True, padding_value=0)\n",
    "padded_target = pad_sequence(target_tensors, batch_first=True, padding_value=0)\n",
    "\n",
    "# Create a TensorDataset using the padded tensors\n",
    "train_dataset = TensorDataset(padded_src, padded_target)\n",
    "train_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch 1/10\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/31 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "not enough values to unpack (expected 4, got 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[93]\u001b[39m\u001b[32m, line 23\u001b[39m\n\u001b[32m     20\u001b[39m model.train()\n\u001b[32m     21\u001b[39m total_loss = \u001b[32m0\u001b[39m\n\u001b[32m---> \u001b[39m\u001b[32m23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m src, tgt, src_mask, tgt_mask \u001b[38;5;129;01min\u001b[39;00m tqdm(train_loader):\n\u001b[32m     24\u001b[39m     src, tgt, src_mask, tgt_mask = src.to(device), tgt.to(device), src_mask.to(device), tgt_mask.to(device)\n\u001b[32m     26\u001b[39m     \u001b[38;5;66;03m# Shift target for decoder input/output\u001b[39;00m\n",
      "\u001b[31mValueError\u001b[39m: not enough values to unpack (expected 4, got 2)"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Model\n",
    "src_vocab_size = len(flattened_src)\n",
    "target_vocab_size = len(flattened_target)\n",
    "model = make_model(src_vocab_size, target_vocab_size, 1).to(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "\n",
    "# Loss & Optimizer\n",
    "loss_fn = nn.NLLLoss(ignore_index=tokenizer.pad_token_id)\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-4)\n",
    "\n",
    "# Training\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "epochs = 10\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    print(f\"\\nEpoch {epoch+1}/{epochs}\")\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for src, tgt, src_mask, tgt_mask in tqdm(train_loader):\n",
    "        src, tgt, src_mask, tgt_mask = src.to(device), tgt.to(device), src_mask.to(device), tgt_mask.to(device)\n",
    "\n",
    "        # Shift target for decoder input/output\n",
    "        tgt_input = tgt[:, :-1]\n",
    "        tgt_output = tgt[:, 1:]\n",
    "        tgt_len = tgt_input.size(1)\n",
    "        tgt_mask = subsequent_mask(tgt_len).to(device)      # shape: [1, tgt_len, tgt_len]\n",
    "        tgt_mask = tgt_mask.unsqueeze(1)\n",
    "\n",
    "        out = model(src, tgt_input, src_mask, tgt_mask)\n",
    "        logits = model.generator(out)\n",
    "\n",
    "        logits = logits.view(-1, logits.size(-1))\n",
    "        tgt_output = tgt_output.contiguous().view(-1)\n",
    "\n",
    "        loss = loss_fn(logits, tgt_output)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "\n",
    "    avg_loss = total_loss / len(train_loader)\n",
    "    print(f\"Average loss: {avg_loss:.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
